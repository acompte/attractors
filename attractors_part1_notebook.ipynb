{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c301a",
   "metadata": {},
   "source": [
    "# Single neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecff0b7",
   "metadata": {},
   "source": [
    "![experiment schematic](figs/single_cell_recording.png \"Single cell recording\")\n",
    "![simplified system](figs/singe_cell_sim.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09bd15",
   "metadata": {},
   "source": [
    "This simplified model of a neuron is mathematically expressed with a first-order ordinary differential equation:\n",
    "\n",
    "$$\n",
    "\\tau \\frac{d r(t)}{dt} = -r(t) + I(t)\n",
    "$$\n",
    "\n",
    "which we can discretize in time (in steps $dt$) in order to simulate numerically using the Euler method:\n",
    "\n",
    "$\\tau \\frac{r(t+dt) - r(t)}{dt} = -r(t) + I(t) \\; \\; \\; $ so that:  $\\; \\; \\; r(t+dt) = r(t) + \\frac{dt}{\\tau} \\left[ -r(t) + I(t) \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b92ab9b",
   "metadata": {},
   "source": [
    "Here is a piece of code that simulates how a neuron with membrane time constant $\\tau$ integrates a time varying input and generates a rate output $r(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8651f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt = 0.001 # time step, in s\n",
    "time = np.arange(0, 5, dt) # step times of the whole simulation, in s\n",
    "Nt = len(time) # total number of steps in simulation\n",
    "T = 0.5 # period of input current, in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8166d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 10*np.ones((Nt,))  # input current\n",
    "# input = 1 + np.sin(2*np.pi*time/T) # input current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b628433",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = np.ones((Nt,)) # initialize firing rate at 1\n",
    "\n",
    "tau = 0.02 # membrane time constant, in s\n",
    "\n",
    "# simulation loop, through all time steps\n",
    "for i in range(Nt-1):\n",
    "    rate[i+1] = rate[i] + dt/tau*(-rate[i] + input[i])\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(nrows=2)\n",
    "ax[0].plot(time, input, 'k')\n",
    "ax[0].set_title('Input')\n",
    "ax[0].set_ylabel('current')\n",
    "ax[1].plot(time, rate, 'r')\n",
    "ax[1].set_title('Output')\n",
    "ax[1].set_xlabel('time (s)')\n",
    "ax[1].set_ylabel('rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba93b2a",
   "metadata": {},
   "source": [
    "Now change the code above to see how a neuron with a much longer time constant (e.g. $\\tau = 1 s$) would integrate this same input. What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75ffcb",
   "metadata": {},
   "source": [
    "Now compare again the two neurons, one with short time constant ($\\tau = 0.02 s$) and one with very long time constant ($\\tau = 10 s$), but now have them respond to an input representing a current pulse, as per the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.ones((Nt,))\n",
    "pulsesat = int(Nt/6)\n",
    "input[pulsesat] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b49dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc450133",
   "metadata": {},
   "source": [
    "What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca3f56",
   "metadata": {},
   "source": [
    "Now do the same thing but for an input consisting in a train of successive current pulses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.ones((Nt,))\n",
    "pulsesat = int(Nt/6)*np.array([1,2,3,4,5])\n",
    "input[pulsesat] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b9261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "730a4961",
   "metadata": {},
   "source": [
    "This is quite exciting: a single neuron can do things such as memory, or stimulus counting (integration)... but, what is a realistic membrane time constant in the brain? 20ms!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d57b9",
   "metadata": {},
   "source": [
    "How could the brain do memory or stimulus integration if neurons have such slow time constant? Networks! Here is the simplest possible network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759d84f",
   "metadata": {},
   "source": [
    "# Neuron with autapse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6ed4b",
   "metadata": {},
   "source": [
    "![Autapse](figs/autapse.svg \"Autapse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fdb632",
   "metadata": {},
   "source": [
    "Now this simplified network model of a neuron is mathematically expressed with this first-order ordinary differential equation:\n",
    "\n",
    "$$\n",
    "\\tau \\frac{d r(t)}{dt} = -r(t) + J r(t) + I(t)\n",
    "$$\n",
    "\n",
    "where $J$ is the strength of the self-coupling of the neuron with itself (autapse). We again discretize this equation in time (in steps $dt$) using the Euler method to get:\n",
    "\n",
    "$r(t+dt) = r(t) + \\frac{dt}{\\tau} \\left[ (J - 1) r(t) + I(t) \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6833b21",
   "metadata": {},
   "source": [
    "Now try simulating this equation for various values of $J$ (restricted to be non-negative, $J \\ge 0$) when the neuron has a short membrane time constant (keep fixed $\\tau = 0.02s$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23472e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = np.ones((Nt,))\n",
    "\n",
    "tau = 0.02\n",
    "J = 0.99\n",
    "\n",
    "input = np.ones((Nt,))\n",
    "pulsesat = int(Nt/6)\n",
    "input[pulsesat] = 3\n",
    "input = input*np.abs(1-J)\n",
    "\n",
    "for i in range(Nt-1):\n",
    "    rate[i+1] = rate[i] + dt/tau*((J - 1)*rate[i] + input[i])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2)\n",
    "ax[0].plot(time, input, 'k')\n",
    "ax[0].set_title('Input')\n",
    "ax[0].set_ylabel('current')\n",
    "ax[1].plot(time, rate, 'r')\n",
    "ax[1].set_title('Output')\n",
    "ax[1].set_xlabel('time (s)')\n",
    "ax[1].set_ylabel('rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be185c28",
   "metadata": {},
   "source": [
    "you can play with $J$ in the above code, or make a more sophisticated plot using the **Plotly** graphics library that allows you to interact with the plot directly. Here is the code and look at the result in the cell below it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23fc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "dt = 0.001 # time step, in s\n",
    "T = 5 # total time, in s\n",
    "time = np.arange(0, T, dt) # step times of the whole simulation, in s\n",
    "Nt = len(time) # total number of steps in simulation\n",
    "\n",
    "tau = 0.02\n",
    "J = 0.99\n",
    "\n",
    "def get_traces(J):\n",
    "    # initialize inputs \n",
    "    input = np.ones((Nt,))\n",
    "    pulsesat = int(Nt/6)\n",
    "    input[pulsesat] = 3\n",
    "    input = input*np.abs(1-J)\n",
    "    \n",
    "    rate = np.ones((Nt,))\n",
    "\n",
    "    for i in range(Nt-1):\n",
    "        rate[i+1] = rate[i] + dt/tau*((J - 1)*rate[i] + input[i])\n",
    "\n",
    "    return rate, input\n",
    "\n",
    "\n",
    "# Create figure\n",
    "#fig = go.Figure()\n",
    "fig = make_subplots(rows=2, cols=1, row_heights=[0.3, 0.7])\n",
    "\n",
    "# Add traces, one for each slider step\n",
    "for step in np.arange(0.8, 1, 0.005):\n",
    "    rate, input = get_traces(step)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            visible=False,\n",
    "            line=dict(color=\"#FF0000\", width=4),\n",
    "            name=\"\",\n",
    "            x = time,\n",
    "            y = rate),\n",
    "            row = 2, col = 1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            visible=False,\n",
    "            line=dict(color=\"#FF0000\", width=4),\n",
    "            name=\"\",\n",
    "            x = time,\n",
    "            y = input,\n",
    "            showlegend = False),\n",
    "            row = 1, col = 1)\n",
    "\n",
    "# Make 10th trace visible\n",
    "mid = len(fig.data)//2\n",
    "fig.data[mid].visible = True\n",
    "fig.data[mid+1].visible = True\n",
    "\n",
    "# Create and add slider\n",
    "steps = []\n",
    "for i in range(len(fig.data)//2):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig.data)}], \n",
    "        label=str(0.005 * i + 0.8) # layout attribute\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][2*i] = True  # Toggle i'th trace to \"visible\"\n",
    "    step[\"args\"][0][\"visible\"][2*i+1] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=mid/2,\n",
    "    currentvalue={\"prefix\": \"Coupling J: \"},\n",
    "    pad={\"t\": 10},\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    sliders=sliders, height=500, width=600\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"\", range=[0, T], row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"time (s)\", range=[0, T], row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"current\",  row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"rate\",  row=2, col=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49678b24",
   "metadata": {},
   "source": [
    "In what conditions does this network show perfect memory (i.e. it has a different rate depending on whether a stimulus was presented in the past or not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6059fc9",
   "metadata": {},
   "source": [
    "What happens if you set $J>1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845683f",
   "metadata": {},
   "source": [
    "So, through recurrent connections, neurons with short membrane time constants can achieve memory and integration capabilities as if they had long time constants. This is one of the fundamental principles of network function in attractor networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd7584",
   "metadata": {},
   "source": [
    "Now let's address the issues with this simple network function: fine-tuning of memory ($J=1$) and instabilities when $J > 1$. Let us consider a fundamental architectural motif in neural circuits in the cerebral cortex: neurons are either excitatory or inhibitory, and are strongly coupled with one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8964252e",
   "metadata": {},
   "source": [
    "# The E-I circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe5bd0",
   "metadata": {},
   "source": [
    "We thus consider two neurons, one excitatory, one inhibitory, mutually connected. The network model is now mathematically expressed with this system of coupled first-order ordinary differential equations:\n",
    "\n",
    "$$\n",
    "\\tau \\frac{d r_E(t)}{dt} = -r_E(t) + G_E \\left[ W_{EE} r_E(t) - W_{EI} r_I(t) + I_E(t) - \\theta_E \\right]_{+}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tau \\frac{d r_I(t)}{dt} = -r_I(t) + G_I \\left[ W_{IE} r_E(t) - W_{II} r_I(t) + I_I(t) - \\theta_I \\right]_{+} \n",
    "$$\n",
    "\n",
    "where $W_{XY}$ are all non-negative and denote the strengths of the couplings between neuron $X$ and neuron $Y$. To ensure that firing rates are positive, we apply a linear-threshold transformation $G [\\;I - \\theta ]_{+}$ to the inputs, with thresholds $\\theta_E$ and $\\theta_I$. We again discretize this equation in time (in steps $dt$) using the Euler method to get:\n",
    "\n",
    "$$\n",
    "r_E(t+dt) = r_E(t) + \\frac{dt}{\\tau} \\left( -r_E(t) + G_E \\left[ W_{EE} r_E(t) - W_{EI} r_I(t) + I_E(t) - \\theta_E \\right]_+ \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_I(t+dt) = r_I(t) + \\frac{dt}{\\tau} \\left( -r_I(t) + G_I \\left[ W_{IE} r_E(t) -W_{II} r_I(t) + I_I(t) - \\theta_I \\right]_+ \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba61a92",
   "metadata": {},
   "source": [
    "This system now is quite rich and can display a number of interesting dynamics. For instance, it can show the memory function that we discussed, but in a much more robust system. You can play with connectivity strengths in this code and convince you that this perfect memory is not subject to strict fine-tuning, thanks to the interaction of excitation and inhibition. For a detailed analysis of this system dynamics, you can explore [this article ](https://doi.org/10.7554/eLife.22425)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdac1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.001\n",
    "time = np.arange(0, 5, dt)\n",
    "Nt = len(time)\n",
    "\n",
    "input = np.zeros((Nt,))\n",
    "pulsesat = int(Nt/6)\n",
    "input[pulsesat:pulsesat+150] = 0.4\n",
    "\n",
    "tauE = 0.03\n",
    "tauI = 0.01\n",
    "WEE = 5\n",
    "WEI = 1\n",
    "WIE = 10\n",
    "WII = 0.5\n",
    "GE = 1\n",
    "GI = 4\n",
    "\n",
    "rateI = np.zeros((Nt,))\n",
    "rateE = np.zeros((Nt,))\n",
    "\n",
    "def rectify(x, threshold):\n",
    "    return (x - threshold) * (x > threshold)\n",
    "\n",
    "for i in range(Nt-1):\n",
    "    rateE[i+1] = rateE[i] + dt/tauE*( -rateE[i] + GE * rectify(WEE*rateE[i] - WEI*rateI[i] + input[i], 0.35) )\n",
    "    rateI[i+1] = rateI[i] + dt/tauI*( -rateI[i] + GI * rectify(WIE*rateE[i] - WII*rateI[i] , 25) )\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2)\n",
    "ax[0].plot(time, input, 'k')\n",
    "ax[0].set_title('Input')\n",
    "ax[0].set_ylabel('current')\n",
    "ax[1].plot(time, rateE, 'r', label='E-cell')\n",
    "ax[1].plot(time, rateI, 'b', label='I-cell')\n",
    "ax[1].set_title('Output')\n",
    "ax[1].set_xlabel('time (s)')\n",
    "ax[1].set_ylabel('rate')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5aa16",
   "metadata": {},
   "source": [
    "As an exercise, explore in this simulation the results of the phase space analysis shown in class. Try setting $W_{EE}$ outside the range of bistability, or move close and far of the bifurcation and interpret what you see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bec940",
   "metadata": {},
   "source": [
    "Here is the code of the phase space analysis, if you want to generate other analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ee47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "\n",
    "bm.enable_x64()\n",
    "bm.set_platform('cpu')\n",
    "\n",
    "@bp.odeint\n",
    "def int_E(rE, t, rI, input=0.1, WEE=WEE, thrE=3, tauE=tauE, GE=GE):\n",
    "    return - rE / tauE + GE * rectify(WEE*rE - WEI*rI + input, thrE) / tauE\n",
    "\n",
    "@bp.odeint\n",
    "def int_I(rI, t, rE, input=0.1, WIE=WIE, thrI=25, tauI=tauI, GI=GI):\n",
    "    return - rI / tauI + GI * rectify(WIE*rE - WII*rI + input, thrI)  / tauI\n",
    "\n",
    "\n",
    "analyzer = bp.analysis.PhasePlane2D(\n",
    "    model=[int_E, int_I],\n",
    "    target_vars={'rE': [0, 5], 'rI': [0, 20]},\n",
    "    pars_update={'thrE': 1},\n",
    "    # pars_update={'input': 0, 'WEE': 5, 'thrE': 3, 'tauE': 0.25, 'tauI': 0.01, 'GI': 4, 'thrI': 25},\n",
    "    resolutions=0.05,\n",
    ")\n",
    "analyzer.plot_vector_field()\n",
    "analyzer.plot_nullcline(coords=dict(rI='rE-rI'),\n",
    "                        x_style={'fmt': '-'},\n",
    "                        y_style={'fmt': '-'})\n",
    "analyzer.plot_fixed_point()\n",
    "\n",
    "plt.gca().set_box_aspect(1)\n",
    "plt.ylabel('$rate_I$')\n",
    "plt.xlabel('$rate_E$')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502afc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analyzer = bp.analysis.Bifurcation2D(\n",
    "  model=[int_E, int_I],\n",
    "  target_vars={'rE': [0., 10], 'rI': [0., 20.]},\n",
    "#  target_pars={'thrE': [2., 15.]},\n",
    "#  resolutions={'thrE': 0.3},\n",
    "  target_pars={'WEE': [2., 6.]},\n",
    "  resolutions={'WEE': 0.05},\n",
    ")\n",
    "\n",
    "analyzer.plot_bifurcation(num_rank=50)\n",
    "\n",
    "plt.close();\n",
    "plt.gca().set_box_aspect(1)\n",
    "plt.xlabel('WEE')\n",
    "plt.ylabel('$rate_E$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ced54",
   "metadata": {},
   "source": [
    "Again, here we have the interactive plot to explore the effect of $G_E$ on the stability of the network activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3390169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.001 # time step, in s\n",
    "T = 1 # total time, in s\n",
    "time = np.arange(0, T, dt) # step times of the whole simulation, in s\n",
    "Nt = len(time) # total number of steps in simulation\n",
    "\n",
    "input = np.zeros((Nt,))\n",
    "pulsesat = int(Nt/3)\n",
    "input[pulsesat:pulsesat+50] = 0.4\n",
    "\n",
    "tauE = 0.03\n",
    "tauI = 0.01\n",
    "WEE = 5\n",
    "WEI = 1\n",
    "WIE = 10\n",
    "WII = 0.5\n",
    "GE = 1\n",
    "GI = 4\n",
    "\n",
    "\n",
    "def rectify(x, threshold):\n",
    "    return (x - threshold) * (x > threshold)\n",
    "\n",
    "def get_traces(GE):\n",
    "    \n",
    "    rateI = np.zeros((Nt,))\n",
    "    rateE = np.zeros((Nt,))\n",
    "\n",
    "    for i in range(Nt-1):\n",
    "        rateE[i+1] = rateE[i] + dt/tauE*( -rateE[i] + GE * rectify(WEE*rateE[i] - WEI*rateI[i] + input[i], 0.35) )\n",
    "        rateI[i+1] = rateI[i] + dt/tauI*( -rateI[i] + GI * rectify(WIE*rateE[i] - WII*rateI[i] , 25) )\n",
    "\n",
    "    return rateE, rateI, input\n",
    "\n",
    "\n",
    "# Create figure\n",
    "#fig = go.Figure()\n",
    "fig = make_subplots(rows=2, cols=1, row_heights=[0.3, 0.7])\n",
    "\n",
    "# Add traces, one for each slider step\n",
    "for step in np.arange(0.5, 1.5, 0.005):\n",
    "    rateE, rateI, input = get_traces(step)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            visible=False,\n",
    "            line=dict(color=\"#FF0000\", width=4),\n",
    "            name=\"E-pop\",\n",
    "            x = time,\n",
    "            y = rateE),\n",
    "            row = 2, col = 1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            visible=False,\n",
    "            line=dict(color=\"#0000FF\", width=4),\n",
    "            name=\"I-pop\",\n",
    "            x = time,\n",
    "            y = rateI),\n",
    "            row = 2, col = 1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            visible=False,\n",
    "            line=dict(color=\"#000000\", width=4),\n",
    "            name=\"\",\n",
    "            x = time,\n",
    "            y = input,\n",
    "            showlegend = False),\n",
    "            row = 1, col = 1)\n",
    "\n",
    "# Make 10th trace visible\n",
    "mid = len(fig.data)//3\n",
    "fig.data[mid].visible = True\n",
    "fig.data[mid+1].visible = True\n",
    "fig.data[mid+2].visible = True\n",
    "\n",
    "# Create and add slider\n",
    "steps = []\n",
    "for i in range(len(fig.data)//3):\n",
    "    step = dict(\n",
    "        method=\"update\",\n",
    "        args=[{\"visible\": [False] * len(fig.data)}], \n",
    "        label=str(np.around(0.005 * i + 0.5, 2)) # layout attribute\n",
    "    )\n",
    "    step[\"args\"][0][\"visible\"][3*i] = True  # Toggle i'th trace to \"visible\"\n",
    "    step[\"args\"][0][\"visible\"][3*i+1] = True  # Toggle i'th trace to \"visible\"\n",
    "    step[\"args\"][0][\"visible\"][3*i+2] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active=mid/2,\n",
    "    currentvalue={\"prefix\": \"Coupling GE: \"},\n",
    "    pad={\"t\": 10},\n",
    "    steps=steps\n",
    ")]\n",
    "\n",
    "fig.update_layout(\n",
    "    sliders=sliders, height=500, width=600\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"\", range=[0, T], row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"time (s)\", range=[0, T], row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"current\",  row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"rate\",  row=2, col=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc96f06c",
   "metadata": {},
   "source": [
    "## Advanced topic 1: Inhibition-stabilized network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bf828c",
   "metadata": {},
   "source": [
    "How does this network respond to an excitatory input pulse applied to the excitatory population? and to the inhibitory population? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fcf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.0005\n",
    "time = np.arange(0, 5, dt)\n",
    "Nt = len(time)\n",
    "\n",
    "# initialize all inputs to zero\n",
    "inputE = np.zeros((Nt,))\n",
    "inputI = np.zeros((Nt,))\n",
    "\n",
    "# first pulse to go into the non-zero solution\n",
    "pulsesat = int(Nt/6)\n",
    "inputE[pulsesat:pulsesat+150] = 0.4 \n",
    "\n",
    "# now let's apply a pulse in the middle of the simulation to either the excitatory or the inhibitory neuron\n",
    "# PLAY WITH THE TWO POSSIBLE INPUTS inputI AND inputE\n",
    "pulsesat = int(3*Nt/6)\n",
    "inputI[pulsesat:pulsesat+300] = 4\n",
    "#inputE[pulsesat:pulsesat+300] = 4\n",
    "\n",
    "tauE = 0.01\n",
    "tauI = 0.002\n",
    "WEE = 5\n",
    "WEI = 1\n",
    "WIE = 10\n",
    "WII = 0.5\n",
    "GE = 1\n",
    "GI = 4\n",
    "\n",
    "rateI = np.zeros((Nt,))\n",
    "rateE = np.zeros((Nt,))\n",
    "\n",
    "def rectify(x, threshold):\n",
    "    return (x - threshold) * (x > threshold)\n",
    "\n",
    "for i in range(Nt-1):\n",
    "    rateE[i+1] = rateE[i] + dt/tauE*( -rateE[i] + GE * rectify(WEE*rateE[i] - WEI*rateI[i] + inputE[i], 0.35) )\n",
    "    rateI[i+1] = rateI[i] + dt/tauI*( -rateI[i] + GI * rectify(WIE*rateE[i] - WII*rateI[i] + inputI[i], 25) )\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2)\n",
    "ax[0].plot(time, inputE, 'r')\n",
    "ax[0].plot(time, inputI, 'b')\n",
    "ax[0].set_title('InputE')\n",
    "ax[0].set_ylabel('current')\n",
    "ax[1].plot(time, rateE, 'r', label='E-cell')\n",
    "ax[1].plot(time, rateI, 'b', label='I-cell')\n",
    "ax[1].set_title('Output')\n",
    "ax[1].set_xlabel('time (s)')\n",
    "ax[1].set_ylabel('rate')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c796b225",
   "metadata": {},
   "source": [
    "What do you see? Do you see a paradox? This network regime has been named \"inhibition-stabilized network\" or ISN. You can learn more about it in [this article](https://doi.org/10.1016/j.neuron.2009.03.028). This regime of operation is now considered to apply quite generally to neural circuits in the cerebral cortex (see [this article](https://doi.org/10.7554/eLife.54875))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9182e16d",
   "metadata": {},
   "source": [
    "If you were wondering, there are other EI networks very similar to this one that do not behave in this paradoxical way. Take a look for instance to the classic Wilson-Cowan implementation of the EI network:\n",
    "\n",
    "$$\n",
    "\n",
    "\\small \\tau_E \\frac{d r_E(t)}{dt} = \\small -r_E(t) + [1-r_E(t)] F_E\\left[ W_{EE} r_E(t) - W_{EI} r_I(t) \\right] \\\\\n",
    "\n",
    "\\small \\tau_I \\frac{d r_I(t)}{dt} = \\small -r_I(t) + [1-r_I(t)] F_I\\left[ W_{IE} r_E(t) - W_{II} r_I(t) \\right] \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "\n",
    "dt = 0.001 # time step, in s\n",
    "T = 3 # total time, in s\n",
    "time = np.arange(0, T, dt) # step times of the whole simulation, in s\n",
    "Nt = len(time) # total number of steps in simulation\n",
    "\n",
    "\n",
    "WEE = 12\n",
    "WEI = 4\n",
    "WIE = 13\n",
    "WII = 11\n",
    "\n",
    "tauE = 0.01\n",
    "tauI = 0.01\n",
    "\n",
    "GE = 1.2\n",
    "GI = 1\n",
    "\n",
    "thrE = 2.8\n",
    "thrI = 4\n",
    "\n",
    "def F (x, a, theta):\n",
    "    return 1 / (1 + bm.exp(-a * (x - theta))) - 1 / (1 + bm.exp(a * theta))\n",
    "\n",
    "def get_traces(inp_to=0):\n",
    "    inputE = np.zeros((Nt,))\n",
    "    pulsesat = int(Nt/3)\n",
    "    inputE[pulsesat:pulsesat+50] = 4\n",
    "    inputI = np.zeros((Nt,))\n",
    "    inputI[pulsesat:pulsesat+50] = 4\n",
    "\n",
    "    pulsesat = int(2*Nt/3)\n",
    "    if inp_to==1:\n",
    "        inputE[pulsesat:pulsesat+200] = 4\n",
    "    elif inp_to==-1:\n",
    "        inputI[pulsesat:pulsesat+200] = 2\n",
    "    \n",
    "    rateI = np.zeros((Nt,))\n",
    "    rateE = np.zeros((Nt,))\n",
    "\n",
    "    for i in range(Nt-1):\n",
    "        rateE[i+1] = rateE[i] + dt/tauE*( -rateE[i] + (1 - rateE[i]) * F(WEE*rateE[i] - WEI*rateI[i] + inputE[i], GE, thrE) )\n",
    "        rateI[i+1] = rateI[i] + dt/tauI*( -rateI[i] + (1 - rateI[i]) * F(WIE*rateE[i] - WII*rateI[i] + inputI[i], GI, thrI) )\n",
    "\n",
    "    return rateE, rateI, inputE, inputI\n",
    "\n",
    "\n",
    "# Create figure\n",
    "#fig = go.Figure()\n",
    "fig = make_subplots(rows=2, cols=1, row_heights=[0.3, 0.7], shared_xaxes=True)\n",
    "\n",
    "# Add traces, one for each slider step\n",
    "for step in list([-1, 0, 1]):\n",
    "    rateE, rateI, inputE, inputI = get_traces(step)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            visible=False,\n",
    "            line=dict(color=\"#FF0000\", width=4),\n",
    "            name=\"E-pop\",\n",
    "            x = time,\n",
    "            y = rateE,\n",
    "            showlegend = False),\n",
    "            row = 2, col = 1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            visible=False,\n",
    "            line=dict(color=\"#0000FF\", width=4),\n",
    "            name=\"I-pop\",\n",
    "            x = time,\n",
    "            y = rateI,\n",
    "            showlegend = False),\n",
    "            row = 2, col = 1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            visible=False,\n",
    "            line=dict(color=\"#550000\", width=4),\n",
    "            name=\"\",\n",
    "            x = time,\n",
    "            y = inputE,\n",
    "            showlegend = False),\n",
    "            row = 1, col = 1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            visible=False,\n",
    "            line=dict(color=\"#000055\", width=4),\n",
    "            name=\"\",\n",
    "            x = time,\n",
    "            y = inputI,\n",
    "            showlegend = False),\n",
    "            row = 1, col = 1)\n",
    "\n",
    "# Make 10th trace visible\n",
    "mid = len(fig.data)//3\n",
    "fig.data[mid].visible = True\n",
    "fig.data[mid+1].visible = True\n",
    "fig.data[mid+2].visible = True\n",
    "fig.data[mid+3].visible = True\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400, width=500,\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type=\"buttons\",\n",
    "            direction=\"right\",\n",
    "            active=0,\n",
    "            x=0.57,\n",
    "            y=1.2,\n",
    "            buttons=list([\n",
    "                dict(label=\"None\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [False, False, False, False, True, True, True, True, False, False, False, False]},\n",
    "                           {\"title\": \"No stim\"}]),\n",
    "                dict(label=\"StimE\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [False, False, False, False, False, False, False, False, True, True, True, True]},\n",
    "                           {\"title\": \"Stim E\"}]),\n",
    "                dict(label=\"StimI\",\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [True, True, True, True, False, False, False, False, False, False, False, False]},\n",
    "                           {\"title\": \"Stim I\"}]),\n",
    "            ]),\n",
    "        )\n",
    "    ])\n",
    "\n",
    "fig.update_xaxes(title_text=\"\", range=[0, T], row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"time (s)\", range=[0, T], row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"current\",  row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"rate\",  row=2, col=1)\n",
    "\n",
    "config = {'displayModeBar': False}\n",
    "\n",
    "fig.show(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d28ebe",
   "metadata": {},
   "source": [
    "# Double-well attractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd551bf",
   "metadata": {},
   "source": [
    "![](figs/doublewell.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63725b",
   "metadata": {},
   "source": [
    "Now this starts getting complicated. Following the previous schematic for the EI network above, you could certainly write the code for this 3-neuron network (in practice, we think of each unit in the network as a population of neurons, so we talk of a 3-population network). This is an exercise for you to try on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a8a4c",
   "metadata": {},
   "source": [
    "However, we usually simplify things to deal with only two dynamical variables. Here, we can take advantage of the fact that inhibitory neurons have a faster time constant to assume that their equation relaxes more quickly to the steady state, so we freeze the firing rate of the inhibitory population to the steady state of its dynamics given the momentary rates of the two excitatory populations. If you are interested in these derivations, you can check them in [this book chapter](https://neuronaldynamics.epfl.ch/online/Ch16.S3.html). The resulting equations are:\n",
    "\n",
    "$$\n",
    "\\tau_E \\frac{d I_{E,1}}{dt} = - I_{E1} + (W_{EE} - \\alpha) g_E(I_{E1}) - \\alpha g_E(I_{E2}) + S_{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tau_E \\frac{d I_{E,2}}{dt} = - I_{E2} + (W_{EE} - \\alpha) g_E(I_{E2}) - \\alpha g_E(I_{E1}) + S_{2} \n",
    "$$\n",
    "\n",
    "where $\\alpha = -\\gamma W_{EI}W_{IE}$ represents the effective inhibitory coupling between excitatory populations (via the inhibitory population), and $S_1$/$S_2$ are the inputs arriving to each excitatory population. We can simplify the parametrization of these equations by defining $J_E=W_{EE}-\\alpha$ and $J_I=\\alpha$ to get the [Wong and Wang model](https://doi.org/10.1523/JNEUROSCI.3733-05.2006):\n",
    "\n",
    "\n",
    "$$\n",
    "\\tau_E \\frac{d I_{E,1}}{dt} = - I_{E1} + J_E g_E(I_{E1}) - J_I g_E(I_{E2}) + S_{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tau_E \\frac{d I_{E,2}}{dt} = - I_{E2} + J_E g_E(I_{E2}) - J_I g_E(I_{E1}) + S_{2} \n",
    "$$\n",
    "\n",
    "Note that this model formulation is slightly different to what we have been doing so far in this workshop: now our dynamical variables are the inputs and not the rates of the populations. To plot the rates, we have to use $r_{E} = g_E(I_E)$.\n",
    "\n",
    "These equations can be discretized to obtain this simulation code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.0002\n",
    "time = np.arange(0, 10, dt)\n",
    "Nt = len(time)\n",
    "\n",
    "# initialize inputs \n",
    "drive0 = 0.05\n",
    "inputE1 = drive0*np.ones((Nt,))\n",
    "inputE2 = drive0*np.ones((Nt,))\n",
    "\n",
    "\n",
    "# current pulse into one of the two populations\n",
    "pulsesat = int(Nt/6)\n",
    "inputE1[pulsesat:pulsesat+150] = 0.4 \n",
    "\n",
    "tauE = 0.01\n",
    "JE = 1.1\n",
    "JI = 1.8\n",
    "\n",
    "inpE1 = -0.005*np.ones((Nt,))\n",
    "inpE2 = -0.005*np.ones((Nt,))\n",
    "\n",
    "def curr_to_rate(x):\n",
    "    return (1+np.tanh(x-0.5))/2\n",
    "\n",
    "for i in range(Nt-1):\n",
    "    inpE1[i+1] = inpE1[i] + dt/tauE*(-inpE1[i] + JE*curr_to_rate(inpE1[i]) - JI*curr_to_rate(inpE2[i]) + inputE1[i] )\n",
    "    inpE2[i+1] = inpE2[i] + dt/tauE*(-inpE2[i] + JE*curr_to_rate(inpE2[i]) - JI*curr_to_rate(inpE1[i]) + inputE2[i] )\n",
    "\n",
    "rateE1 = curr_to_rate(inpE1)\n",
    "rateE2 = curr_to_rate(inpE2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2)\n",
    "ax[0].plot(time, inputE1, 'r')\n",
    "ax[0].plot(time, inputE2, 'm')\n",
    "ax[0].set_title('InputE')\n",
    "ax[0].set_ylabel('current')\n",
    "ax[1].plot(time, rateE1, 'r', label='Population 1')\n",
    "ax[1].plot(time, rateE2, 'm', label='Population 2')\n",
    "ax[1].set_title('Output')\n",
    "ax[1].set_xlabel('time (s)')\n",
    "ax[1].set_ylabel('rate')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f4329",
   "metadata": {},
   "source": [
    "What happens if you send the input current pulse to population 2 instead of population 1? Argue about how this model can be used to remember specific events that happened in the recent past. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35745d9c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fafd1e82",
   "metadata": {},
   "source": [
    "These discrete attractor models have been applied to model working memory for objects. In this case there are just two possible memories, because there are two populations, but this can be generalized to an arbitrary number of discrete memories. If you want to learn more about these models, you can check [this paper](https://www.nature.com/articles/s41586-019-0919-7) on experimental evidence supporting it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dd92c",
   "metadata": {},
   "source": [
    "Also, notice how this network now responds to input by establishing a competition between the two populations: if one wins, the other one loses. This is what we call \"winner-take-all\" dynamics and it is thought also to be a mechanism underlying multiple brain computations from sensory perception to decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f600d41",
   "metadata": {},
   "source": [
    "Here is the code for the phase space analysis of this discrete memory network. Explore how the first graph changes with the parameter `drive` (a uniform input to all the network) and use this to make sense of the second graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d313052",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = drive0\n",
    "\n",
    "@bp.odeint\n",
    "def int_r1(r1, t, r2, drive=drive):\n",
    "    fct = 2*r1*(1.-r1)/tauE\n",
    "    cnv = 0.5 + bm.atanh(2*r1 - 1.)\n",
    "    return (- cnv + JE*r1 - JI*r2 + drive) *fct\n",
    "\n",
    "@bp.odeint\n",
    "def int_r2(r2, t, r1, drive=drive):\n",
    "    fct = 2*r2*(1.-r2)/tauE\n",
    "    cnv = 0.5 + bm.atanh(2*r2 - 1.)\n",
    "    return (- cnv + JE*r2 - JI*r1 + drive) *fct\n",
    "\n",
    "\n",
    "analyzer = bp.analysis.PhasePlane2D(\n",
    "    model=[int_r1, int_r2],\n",
    "    target_vars={'r1': [0, 1], 'r2': [0, 1]},\n",
    "    # pars_update={'drive': -0.5},\n",
    "    resolutions=0.0005,\n",
    ")\n",
    "analyzer.plot_vector_field()\n",
    "analyzer.plot_nullcline(coords=dict(r2='r2-r1'),\n",
    "                        x_style={'fmt': '-'},\n",
    "                        y_style={'fmt': '-'})\n",
    "analyzer.plot_fixed_point()\n",
    "plt.gca().set_box_aspect(1)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bp.odeint\n",
    "def int_s1(s1, t, s2, drive=drive):\n",
    "    crE1 = (1 + bm.tanh(s1 - 0.5))/2.\n",
    "    crE2 = (1 + bm.tanh(s2 - 0.5))/2.\n",
    "    return - s1 / tauE + JE*crE1 / tauE - JI*crE2 / tauE + drive / tauE\n",
    "\n",
    "@bp.odeint\n",
    "def int_s2(s2, t, s1, drive=drive):\n",
    "    crE1 = (1 + bm.tanh(s1 - 0.5))/2.\n",
    "    crE2 = (1 + bm.tanh(s2 - 0.5))/2.\n",
    "    return - s2 / tauE + JE*crE2 / tauE - JI*crE1 / tauE + drive / tauE\n",
    "\n",
    "analyzer = bp.analysis.Bifurcation2D(\n",
    "  model=[int_s1, int_s2],\n",
    "  target_vars={'s1': [-1.5, 1.5], 's2': [-1.5, 1.5]},\n",
    "  target_pars={'drive': [-0.2, 0.5]},\n",
    "  resolutions={'drive': 0.01},\n",
    ")\n",
    "\n",
    "analyzer.plot_bifurcation(num_rank=50)\n",
    "\n",
    "plt.close();\n",
    "plt.gca().set_box_aspect(1)\n",
    "plt.xlabel('drive')\n",
    "plt.ylabel('current E1 (s1)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021bf672",
   "metadata": {},
   "source": [
    "One very useful way to think about this dynamics is to visualize it as the evolution of a ball that bounces down in a hilly landscape. This is more than just a visual analogy for networks that satisfy perfect symmetry (i.e. the connection from neuron X to neuron Y is equal to the connection from neuron Y to neuron X). For these kinds of networks this analogy is mathematically exact (see for example an explanation [here](https://neuronaldynamics.epfl.ch/online/Ch16.S4.html)) and this hilly landscape is what we call \"energy\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0195f",
   "metadata": {},
   "source": [
    "The energy landscape for this network can be derived analytically as described by [Gerstner et al, 2014](https://neuronaldynamics.epfl.ch/online/Ch16.S4.html):\n",
    "\n",
    "$$\n",
    "E(r_1, r_2) = -\\frac{J_E}{2} (r_1^2 + r_2^2) + J_I r_1 r_2 - S (r_1 + r_2) + \\int_0^{r_1} g_E^{-1}(x) dx + \\int_0^{r_2} g_E^{-1}(x) dx\n",
    "$$\n",
    "\n",
    "Here are two different visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorscale = \"Viridis\"\n",
    "\n",
    "np.seterr(divide = 'ignore') \n",
    "\n",
    "def integral(rate):\n",
    "    lim = 2*rate - 1\n",
    "    return 0.5*rate + 0.5* (lim*np.arctanh(lim) + 0.5*np.log(np.abs(1-lim*lim)) -0.7);\n",
    "\n",
    "def get_energy(drive1, drive2):\n",
    "    energy = np.zeros((100,100))    \n",
    "    rate1 = np.linspace(0,1,100)\n",
    "    rate2 = np.linspace(0,1,100)\n",
    "    for i, r1 in enumerate(rate1):\n",
    "        for j, r2 in enumerate(rate2):\n",
    "            energy[i,j] = -0.5*JE*(r1**2 + r2**2) + JI*r1*r2 - (drive1*r1 + drive2*r2) + integral(r1) + integral(r2)\n",
    "\n",
    "    return energy, rate1, rate2\n",
    "\n",
    "energy, rate1, rate2 = get_energy(drive, drive)\n",
    "\n",
    "# create figure\n",
    "layout = go.Layout(hovermode=False)\n",
    "fig = go.Figure(layout=layout)\n",
    "\n",
    "# Add surface trace\n",
    "potential=energy\n",
    "potential[potential>0.25]=np.nan\n",
    "fig.add_trace(go.Surface(z=potential, \n",
    "    x=rate1, \n",
    "    y=rate2,\n",
    "    contours = {\n",
    "        \"z\": {\"show\": True, \"start\": -0.15, \"end\": 0.25, \"size\": 0.01, \"color\":\"white\"}\n",
    "    },\n",
    "    colorscale=colorscale,\n",
    "    colorbar_title_text='energy'))\n",
    "\n",
    "fig.update_traces(contours_z=dict(show=True, usecolormap=True,highlightcolor=\"limegreen\", project_z=False))\n",
    "\n",
    "# Update plot sizing\n",
    "fig.update_layout(\n",
    "    width=500, height=450,\n",
    "    autosize=False,\n",
    "    margin=dict(t=0, b=0, l=0, r=0),\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "# Update 3D scene options\n",
    "fig.update_scenes(\n",
    "    aspectratio=dict(x=1, y=1, z=0.7),\n",
    "    aspectmode=\"manual\"\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_scenes(xaxis_title_text='rate population 1',  \n",
    "                  yaxis_title_text='rate population 2',  \n",
    "                  zaxis_title_text='energy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integral(rate):\n",
    "    lim = 2*rate - 1\n",
    "    return 0.5*rate + 0.5* (lim*np.arctanh(lim) + 0.5*np.log(np.abs(1-lim*lim)) -0.7);\n",
    "\n",
    "def get_energy(drive1, drive2):\n",
    "    energy = np.zeros((100,100))    \n",
    "    rate1 = np.linspace(0,1,100)\n",
    "    rate2 = np.linspace(0,1,100)\n",
    "    for i, r1 in enumerate(rate1):\n",
    "        for j, r2 in enumerate(rate2):\n",
    "            energy[i,j] = -0.5*JE*(r1**2 + r2**2) + JI*r1*r2 - (drive1*r1 + drive2*r2) + integral(r1) + integral(r2)\n",
    "\n",
    "    return energy, rate1, rate2\n",
    "\n",
    "layout = go.Layout(hovermode=False)\n",
    "fig = go.Figure(layout=layout)\n",
    "      \n",
    "energy, rateE1, rateE2 = get_energy(drive, drive)\n",
    "# energy[energy>-0.15]=np.nan\n",
    "fig.add_trace(\n",
    "        go.Contour(\n",
    "            visible=True,\n",
    "            x = rateE1,\n",
    "            y = rateE2,\n",
    "            z = energy,\n",
    "            contours = {\"start\": -0.3025, \"end\": -0.276, \"size\": 0.002, \"coloring\":\"lines\"},\n",
    "            colorscale=colorscale,\n",
    "            colorbar_title_text='energy'\n",
    "            )\n",
    "        )\n",
    "\n",
    "fig.update_layout(height=450, width=500,\n",
    "    autosize=False,\n",
    "    margin=dict(t=0, b=0, l=0, r=0),\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_xaxes(title_text=\"rate 1\", range=[0, 1])\n",
    "fig.update_yaxes(title_text=\"rate 2\", range=[0, 1])\n",
    "fig.update_scenes(xaxis_title_text='rate 1',  \n",
    "                  yaxis_title_text='rate 2')\n",
    "\n",
    "config = {'displayModeBar': False}\n",
    "\n",
    "fig.show(config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969fc96",
   "metadata": {},
   "source": [
    "## The double-well model in decision making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73689758",
   "metadata": {},
   "source": [
    "Now let's explore this network in a different situation: we do not establish a difference between the external inputs to the two populations but we add random independent noise to the currents of the two populations at each time step in the simulated dynamics. This represents internal noise of the brain. Explore what happens in this simulation by running it repeatedly over several \"trials\". Notice also that we change the overall drive to the network during the simulation, but keeping it equal for the two populations. What does this change in external drive achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86386f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.0005\n",
    "time = np.arange(0, 10, dt)\n",
    "Nt = len(time)\n",
    "\n",
    "# initialize inputs \n",
    "drive0 = 0.\n",
    "inputE1 = drive0*np.ones((Nt,))\n",
    "inputE2 = drive0*np.ones((Nt,))\n",
    "\n",
    "drive = 0.1\n",
    "inputE1[Nt//4:] = drive\n",
    "inputE2[Nt//4:] = drive\n",
    "\n",
    "tauE = 0.05\n",
    "JE = 1.1\n",
    "JI = 1.8\n",
    "\n",
    "inpE1 = -0.005*np.ones((Nt,))\n",
    "inpE2 = -0.005*np.ones((Nt,))\n",
    "\n",
    "def curr_to_rate(x):\n",
    "    return (1+np.tanh(x-0.5))/2\n",
    "\n",
    "sigma = 0.3\n",
    "\n",
    "for i in range(Nt-1):\n",
    "    noise = sigma * np.random.randn()\n",
    "    inpE1[i+1] = inpE1[i] + dt/tauE*(-inpE1[i] + JE*curr_to_rate(inpE1[i]) - JI*curr_to_rate(inpE2[i]) + inputE1[i] + noise)\n",
    "    noise = sigma * np.random.randn()\n",
    "    inpE2[i+1] = inpE2[i] + dt/tauE*(-inpE2[i] + JE*curr_to_rate(inpE2[i]) - JI*curr_to_rate(inpE1[i]) + inputE2[i] + noise)\n",
    "\n",
    "rateE1 = curr_to_rate(inpE1)\n",
    "rateE2 = curr_to_rate(inpE2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2)\n",
    "ax[0].plot(time, inputE1, 'r')\n",
    "ax[0].plot(time, inputE2, 'm')\n",
    "ax[0].set_title('InputE')\n",
    "ax[0].set_ylabel('current')\n",
    "ax[1].plot(time, rateE1, 'r', label='Population 1')\n",
    "ax[1].plot(time, rateE2, 'm', label='Population 2')\n",
    "ax[1].set_title('Output')\n",
    "ax[1].set_xlabel('time (s)')\n",
    "ax[1].set_ylabel('rate')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eac9b1",
   "metadata": {},
   "source": [
    "Which of the two populations wins now the competition? How is that determined?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b0d72",
   "metadata": {},
   "source": [
    "The advantage of having just two dynamical variables is that we can visualize the dynamics in the phase space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.0005\n",
    "time = np.arange(0, 10, dt)\n",
    "Nt = len(time)\n",
    "\n",
    "# initialize inputs \n",
    "drive0 = 0.\n",
    "inputE1 = drive0*np.ones((Nt,))\n",
    "inputE2 = drive0*np.ones((Nt,))\n",
    "\n",
    "drive = 0.1\n",
    "inputE1[Nt//4:] = drive\n",
    "inputE2[Nt//4:] = drive\n",
    "\n",
    "tauE = 0.05\n",
    "JE = 1.1\n",
    "JI = 1.8\n",
    "\n",
    "inpE1 = -0.005*np.ones((Nt,))\n",
    "inpE2 = -0.005*np.ones((Nt,))\n",
    "\n",
    "def curr_to_rate(x):\n",
    "    return (1+np.tanh(x-0.5))/2\n",
    "\n",
    "sigma = 0.3\n",
    "\n",
    "for i in range(Nt-1):\n",
    "    noise = sigma * np.random.randn()\n",
    "    inpE1[i+1] = inpE1[i] + dt/tauE*(-inpE1[i] + JE*curr_to_rate(inpE1[i]) - JI*curr_to_rate(inpE2[i]) + inputE1[i] + noise)\n",
    "    noise = sigma * np.random.randn()\n",
    "    inpE2[i+1] = inpE2[i] + dt/tauE*(-inpE2[i] + JE*curr_to_rate(inpE2[i]) - JI*curr_to_rate(inpE1[i]) + inputE2[i] + noise)\n",
    "\n",
    "rateE1 = curr_to_rate(inpE1)\n",
    "rateE2 = curr_to_rate(inpE2)\n",
    "\n",
    "plt.scatter(rateE1, rateE2, c=time, s=4, cmap='cool')\n",
    "plt.colorbar(label='time')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.axline((1, 1), slope=1, color='k', ls='--')\n",
    "plt.xlabel('rate population 2')\n",
    "plt.ylabel('rate population 1');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59d434",
   "metadata": {},
   "source": [
    "try running several times the previous cell and see how the network dynamics changes from trial to trial. Can you make sense of this dynamics? Where does the network start and where does it evolve to towards the end of the trial?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1e389",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02678abd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3edc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide = 'ignore') \n",
    "\n",
    "energy = np.zeros((100,100))\n",
    "\n",
    "def integral(rate):\n",
    "    lim = 2*rate - 1\n",
    "    return 0.5*rate + 0.5* (lim*np.arctanh(lim) + 0.5*np.log(np.abs(1-lim*lim)) -0.7);\n",
    "\n",
    "for i in range(100):\n",
    "    r1 = i/100\n",
    "    for j in range(100):\n",
    "        r2 = j/100\n",
    "        energy[i,j] = -0.5*JE*(r1**2 + r2**2) + JI*r1*r2 - (drive*r1 + drive*r2) + integral(r1) + integral(r2)\n",
    "\n",
    "plt.contour(energy, extent=[0, 1, 0, 1], levels=500)\n",
    "plt.colorbar(label='energy')\n",
    "plt.xlabel('rate population 2')\n",
    "plt.ylabel('rate population 1')\n",
    "\n",
    "\n",
    "plt.scatter(rateE1, rateE2, c=time, s=4, cmap=\"cool\")\n",
    "plt.colorbar(label='time')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.axline((1, 1), slope=1, color='k', ls='--')\n",
    "plt.xlabel('rate population 2')\n",
    "plt.ylabel('rate population 1');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c46248",
   "metadata": {},
   "source": [
    "As you can see, the trajectories in phase space that we saw before are falling down this landscape towards the two \"wells\" of minimal energy. Based on this energy picture, we often call this dynamics \"double-well attractor\", and it is a possible circuit mechanism both for working memory and decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2dca88",
   "metadata": {},
   "source": [
    "The energy landscape can also help us understand what happens in the model when we imbalance the inputs to the two populations. For instance, if the drive to population 2 is stronger than the drive to population 1, simulating the case in which stronger evidence is presented for stimulus 2 than for stimulus 1. This biases the competition between the two populations in favor of population 2 by making the well of population 1 much shallower, or even making it disappear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9240451",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = np.zeros((100,100))\n",
    "drive1 = 0.08\n",
    "drive2 = 0.12\n",
    "\n",
    "def integral(rate):\n",
    "    lim = 2*rate - 1\n",
    "    return 0.5*rate + 0.5* (lim*np.arctanh(lim) + 0.5*np.log(np.abs(1-lim*lim)) -0.7)\n",
    "\n",
    "for i in range(100):\n",
    "    r1 = i/100\n",
    "    for j in range(100):\n",
    "        r2 = j/100\n",
    "        energy[i,j] = -0.5*JE*(r1**2 + r2**2) + JI*r1*r2 - (drive1*r1 + drive2*r2) + integral(r1) + integral(r2)\n",
    "\n",
    "plt.contour(energy, extent=[0, 1, 0, 1], levels=500)\n",
    "plt.colorbar(label='energy')\n",
    "plt.xlabel('rate population 2')\n",
    "plt.ylabel('rate population 1');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5a9ef",
   "metadata": {},
   "source": [
    "If you want to learn more about how this model has been applied to decision making, you can read [this paper](https://www.jneurosci.org/content/26/4/1314.short) by Wong and Wang."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6614d7a3",
   "metadata": {},
   "source": [
    "# Ring attractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9e997b",
   "metadata": {},
   "source": [
    "We have now seen a model that can do interesting computations on a few numbers of discrete items, whether memories or decisions. What about storing a continuous quantity, like a length or an angle? For this purpose, the main model studied in computational neuroscience is the ring attractor model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4d248",
   "metadata": {},
   "source": [
    "![ring model](figs/ring.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d4e3f",
   "metadata": {},
   "source": [
    "In this model, neurons are disposed on a ring and they are connected following a very precise connectivity scheme: neurons being close on the ring excite them more than neurons being more distant on the ring, which eventually inhibit each other effectively. Importantly, this pattern of connectivity is assumed to be translationally invariant, i.e. the strength of the connection between neuron N and neuron N + M  in the ring is exactly the same as the connection between neuron L and neuron L + M, for any N, M and L. Below I show you how to build such a connectivity scheme easily in Python. Looking at the graph below, convince yourself that this connectivity has the properties mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nn = 128\n",
    "x= np.linspace(-np.pi, np.pi, Nn, endpoint=False)\n",
    "\n",
    "cs = np.cos(x)\n",
    "sn = np.sin(x)\n",
    "\n",
    "J0 = -3.2\n",
    "J2 = 8.5\n",
    "J = J0 + J2* (np.outer(cs,cs) + np.outer(sn, sn))\n",
    "\n",
    "plt.imshow(J, origin=\"lower\")\n",
    "plt.xlabel('to neuron')\n",
    "plt.ylabel('from neuron')\n",
    "plt.colorbar(label=\"connection strength\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f43a66",
   "metadata": {},
   "source": [
    "Now let's run the dynamics of a network with this connectivity. Instead of evolving two variables, one for each of two neural populations, as in the double-well model before, we now will evolve a whole vector of rates `rate` containging as many rate variables as neurons in the ring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5cb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=0.01\n",
    "time = np.arange(0,5,dt)\n",
    "Nt = len(time)\n",
    "\n",
    "rate = np.zeros((Nt,Nn)) # vector of rates through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input-output function for all cells, as used previously (Brunel, Cereb Cortex 13:1151, 2003)\n",
    "def fI(x):\n",
    "    return x*x*(x>0)*(x<1) + np.sqrt(np.abs(4*x-3))*(x>=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b69eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = 90 * np.pi/180.\n",
    "\n",
    "I = 4*(1 - 0.1 + 0.1 * np.cos(x - stim)) # stimulus current\n",
    "\n",
    "tau = 0.1\n",
    "\n",
    "for i in range(Nt-1):\n",
    "    if (i>100)&(i<150): \n",
    "        input=I #transient input at the beginning of the simulation\n",
    "    else:\n",
    "        input=0\n",
    "    network_inputs = np.dot(J, rate[i])/Nn # this is the network step, where currents to each neuron are computed from the connectivity J and the rates\n",
    "    rate[i+1] = rate[i] + dt/tau * (-rate[i] + fI(network_inputs + input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef24e03",
   "metadata": {},
   "source": [
    "Let's plot the result of the simulation (vertical dotted lines mark the period of stimulus presentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9da5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3))\n",
    "plt.imshow(rate.T, extent=[0,5,-180,180], aspect=\"auto\", origin=\"lower\")\n",
    "plt.axvline(100*dt, color='w', ls=':')\n",
    "plt.axvline(150*dt, color='w', ls=':')\n",
    "plt.ylabel('neuron (deg)')\n",
    "plt.xlabel('time (s)')\n",
    "plt.colorbar(label='firing rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d26877",
   "metadata": {},
   "source": [
    "Play with the previous code changing: \n",
    "1) the strength of the connectivity $J_2$ (try reducing it to $J_2 = 7$)\n",
    "2) the location where the stimulus is presented (`stim`)\n",
    "\n",
    "What do you observe? Is this always a good system to store continuous memories?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81ff46",
   "metadata": {},
   "source": [
    "If we now take a snapshot of the network activity at the end of the simulation, you can see why we also often call this network a *bump attractor network*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.rad2deg(x), rate[-1]);\n",
    "plt.xlabel('neuron (deg)')\n",
    "plt.ylabel('rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8920c",
   "metadata": {},
   "source": [
    "Also in this network it is interesting to explore how it reacts when it is subject to noisy inputs, simulating the embedding of the circuit in a larger system with many uncontroled and unpredictable components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsims=100\n",
    "dt=0.01\n",
    "time = np.arange(0,5,dt)\n",
    "Nt = len(time)\n",
    "\n",
    "# routine to extract population vectors from matrix of rates\n",
    "vecs = np.cos(x) + 1j*np.sin(x)\n",
    "vecs = np.outer(np.ones((1,Nt)),vecs)\n",
    "def decode(rate):\n",
    "    res = np.sum(vecs*rate, axis=1)\n",
    "    return np.angle(res)\n",
    "\n",
    "endpoints = np.zeros((nsims,))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# run the simulation multiple times\n",
    "for n in range(nsims):\n",
    "\n",
    "    I = 2*np.exp(4*np.cos(x))\n",
    "\n",
    "    rate = np.zeros((Nt,Nn))\n",
    "\n",
    "    tau = 0.1\n",
    "\n",
    "    for i in range(Nt-1):\n",
    "        if (i>100)&(i<150):  \n",
    "            input=I\n",
    "        else:\n",
    "            input=0\n",
    "        network_inputs = np.dot(J, rate[i])/Nn\n",
    "        noise = 0.3*np.random.randn(Nn,1).flatten()\n",
    "        rate[i+1] = rate[i] + dt/tau * (-rate[i] + fI(network_inputs + input + noise))\n",
    "    \n",
    "    trace = decode(rate)\n",
    "\n",
    "    plt.plot(time, np.rad2deg(trace))\n",
    "\n",
    "    endpoints[n] = trace[-1]\n",
    "\n",
    "\n",
    "plt.ylim([-20, 20])\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('neuron (deg)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26ba6e",
   "metadata": {},
   "source": [
    "How do you interpret the results of this simulation? What happens with memories in this network as we make the retention interval or delay period longer and longer? This is a prediction of the model that was effectively validated in neural data of the monkey prefrontal cortex. You can check it out [here](http://neurophysics.ucsd.edu/courses/physics_171/Wimmer_Compte_reprint.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
